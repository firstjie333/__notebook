<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 6.13.3 (455969)"/><meta name="author" content="firstjie333"/><meta name="created" content="2018-04-10 16:07:12 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-12-04 05:51:55 +0000"/><title>DogLeg</title></head><body><div><br/></div><div><br/></div><div><font style="font-size: 24px;"><span style="font-size: 24px;">1.基本知识</span></font></div><div><font style="font-size: 24px;"><hr/></font></div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="background-color: rgb(255, 250, 165); font-weight: bold;-evernote-highlight:true;">1.1  优化问题3个基本要素：优化变量、目标函数、约束条件</span></span></div><div style="box-sizing: border-box; padding: 8px; font-size: 12px; border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.14902);"><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">    X=[x1,x2,…,xn]</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">    min f(X)</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">s.t.    </span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">    f(X) = hk(X)=0 ,k=1,2,…l</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">    f(X) = gk(X)&lt;=0 ,k=1,2,…m</span></div></div><div><br/></div><div>引申：无约束优化、有约束优化、等式约束、不等式约束、线性优化、非线性优化。各类问题都有特定的解决办法</div><div><br/></div><div><span style="background-color: rgb(255, 250, 165); font-weight: bold;-evernote-highlight:true;">1.2 优化问题基本解决办法：解析解法、数值解法</span></div><div><ul><li>解析解法：知道目标函数的具体形式，严格按照数学公式推导求解</li><li>数值解法：拟合思想。工具：泰勒展开(近似)、迭代求解(迭代)</li><ul><li>优化准则法：</li><li>数学规划法：</li></ul></ul></div><div>实际应用中最广泛计算简单的当然是数值解法中的迭代法，也叫数学规划法。</div><div><br/></div><div><span style="background-color: rgb(255, 250, 165); font-weight: bold;-evernote-highlight:true;">1.3 迭代法的3个基本要素</span></div><div><ul><li><span style="font-weight: bold;">初始值、迭代方向、步长</span></li><li>收敛性</li><li>终止条件：</li><ul><li><img src="DogLeg.resources/24F46B69-DF37-45B4-8F76-AEC75DA2A0B2.gif" height="19" width="125"/><br/></li><li><img src="DogLeg.resources/968737F9-3DEC-420A-BF99-49767C5603F4.gif" height="19" width="175"/>  或者 <img src="DogLeg.resources/8C47531F-B041-46D4-AA65-8228394FB934.gif" height="43" width="178"/><br/></li><li>某次迭代点的目标函数的梯度已经到达充分小时：<img src="DogLeg.resources/060C2443-FC13-4744-AF82-814D03E2F334.gif" height="19" width="97"/><br/></li></ul></ul></div><div><br/></div><div><span style="background-color: rgb(255, 250, 165); font-weight: bold;-evernote-highlight:true;">1.4 函数在某一点的梯度方向变换最多上升最快、在负梯度方向下降最快、函数在某一点垂直于梯度方向的方向称为法线方向，将函数分为上升方向和下降方向</span></div><div><ul><li>导数</li><li>偏导数</li><li>方向导数：方向导数是偏导数的推广、偏导数是方向导数的特例</li><li>梯度</li></ul></div><div><br/></div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="background-color: rgb(255, 250, 165); font-weight: bold;-evernote-highlight:true;">1.5 多元函数求函数极小值的条件</span></span></div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="background-color: rgb(255, 250, 165); font-weight: bold;-evernote-highlight:true;">必要条件：函数对Xm处的导数必须为0。=》</span><span style="background-color: rgb(255, 250, 165); font-weight: bold;-evernote-highlight:true;">驻点</span></span></div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="background-color: rgb(255, 250, 165); font-weight: bold;-evernote-highlight:true;">充分条件：G(Xm)或写H(Xm)正定。各阶主子式均&gt;0</span></span></div><div>驻点不一定是极值点，但极值点一定是驻点</div><div><br/></div><div><br/></div><div><br/></div><div><span style="font-size: 24px;">2. </span><span style="font-size: 24px; color: rgb(68, 68, 68); font-family: Tahoma, Arial, Helvetica, sans-serif; font-weight: bold;">无约束优化问题</span></div><div><font style="font-size: 24px;"><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><hr/></span></font></div><div><img src="DogLeg.resources/B4D9A7F5-CB1A-4474-8E32-D1C24C6593BE.gif" height="18" width="236"/> （1）</div><div>min f(X)</div><div><br/></div><div><span style="font-weight: bold;">2.1 最速下降法</span></div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">1.4 函数在某一点的梯度方向变换最多上升最快、在负梯度方向下降最快、函数在某一点垂直于梯度方向的方向称为法线方向，将函数分为上升方向和下降方向</span></div><div style="box-sizing: border-box; padding: 8px; font-size: 12px; border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.14902);"><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">Method1 : 最速下降法</span></div><div><br/></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">1. 选一个初始值X0</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">2. 确定搜索方向= 负梯度方向</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">3. 确定最优步长，f(xk+1) =f(xk+ad) 其中只有a未知，利用求导求极值思想求解a</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">4. 是否满足停止条件</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">5. 否则反复2-5</span></div><div><br/></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">key：1）迭代方向为每一个点的负梯度方向</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">    2)两次相邻的下降方向是相互垂直的，会导致搜索线路是Z型，在接近函数极小值附近搜索速度变慢</span></div></div><div><br/></div><div><span style="font-weight: bold;">2.2 牛顿法</span></div><div>拟合思想：在xk附近领域内用一个二次函数g(x)，即泰勒展开保留到二阶来替代原来的目标函数f(x)。并且将g(x)的极小值作为目标函数f(x)的下一个迭代点。</div><div><img src="DogLeg.resources/884DB69B-02D2-4B7F-ADA7-7BBF6C8F2B8A.gif" height="44" width="593"/><br/></div><div>要求解g(x)的极小值点：参考<span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">1.5 多元函数求函数极小值的条件</span></div><div><span style="font-weight: bold;">通过必要条件求驻点：就是对g(x)求导，求出驻点：</span><img src="DogLeg.resources/55272AD8-4A06-44E6-A246-0A0D8AE188C4.gif" height="21" width="265"/> </div><div>再对比<img src="DogLeg.resources/B4D9A7F5-CB1A-4474-8E32-D1C24C6593BE.gif" height="18" width="236"/> ，所以牛顿法就是 </div><div>搜索方向 = <img src="DogLeg.resources/9CA6AEC1-1EA2-4BBE-BFAA-840C63AEBA96.gif" height="21" width="179"/><br/></div><div>搜索步长 = a = 1</div><div><br/></div><div style="box-sizing: border-box; padding: 8px; font-size: 12px; border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.14902);"><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">Method2 : 牛顿法</span></div><div><br/></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">1. 选一个初始值X0</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">2. 计算gradent 和 hession matrix</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">3. 确定下一个迭代点 </span><img src="DogLeg.resources/55272AD8-4A06-44E6-A246-0A0D8AE188C4.gif" height="21" width="265"/><br/></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">    本质：</span><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">搜索方向 = </span><img src="DogLeg.resources/9CA6AEC1-1EA2-4BBE-BFAA-840C63AEBA96.gif" height="21" width="179"/><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">，搜索步长 = 1</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">4. 是否满足停止条件</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">5. 否则反复2-5</span></div><div><br style="font-family: Monaco; font-size: 12px; color: rgb(51, 51, 51);"/></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">Key:1）迭代方向不一定沿梯度的负方向，没有朝着下降方向搜索的思想，所以对某些非二次项函数，有时候迭代结果会使得函数值上升</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">    2) 计算量大，要</span><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">计算gradent 和 hession matrix 以及 H的逆</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">    3) H 有可能是奇异的，H矩阵不可逆</span></div></div><div><br/></div><div>2.3 阻尼牛顿</div><div style="box-sizing: border-box; padding: 8px; font-size: 12px; border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.14902);"><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">Method3 : 阻尼牛顿法</span></div><div><br style="font-family: Monaco; font-size: 12px; color: rgb(51, 51, 51);"/></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">1. 选一个初始值X0</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">2. 计算gradent 和 hession matrix</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">3. 确定</span><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">搜索方向 = </span><img src="DogLeg.resources/9CA6AEC1-1EA2-4BBE-BFAA-840C63AEBA96.gif" height="21" width="179"/><br/></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">4. 搜索步长 a . 不再为1，而是利用函数极值条件求导求出最优步长</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">5. 是否满足停止条件</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">6. 否则反复2-5</span></div><div><br style="font-family: Monaco; font-size: 12px; color: rgb(51, 51, 51);"/></div></div><div><br/></div><div>2.4 balabala：无约束优化问题，还要好多其他方法，上面的都是间接方法，还有利用目标函数值来就求解的直接方法，大家都懂不多说</div><div><br/></div><div><br/></div><div><span style="font-size: 24px; font-weight: bold;">3. 最小二乘</span></div><div><font style="font-size: 24px;"><hr/></font></div><div><span style="font-weight: bold;">    </span>非线性最小二乘问题来自于非线性回归，即通过观察自变量和因变量数据，求非线性目标函数的系数参数，使得函数模型与观测量尽量相似</div><div><br/></div><div><span style="font-weight: bold;">3.1 我们面临问题举例</span></div><div>    BA问题描述：已经相机的pose、3d点。通过针孔模型我们模型可以算出对应2d点位置，这叫预测。根据匹配我们得到实际的2d点位置，这叫观测。我们的目的是为了调整pose和3d位置，使得优化后的pose和3d点能更好的满足实际观测的情况，使得预测和观测之间的差距不至于太远。</div><div><br/></div><div><br/></div><div><span style="font-weight: bold;">3.2 </span><span style="font-size: 14px; color: rgb(68, 68, 68); font-family: Tahoma, Arial, Helvetica, sans-serif; font-weight: bold;">最小二乘法的目标：求误差的最小平方和</span></div><div style="box-sizing: border-box; padding: 8px; font-size: 12px; border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.14902);"><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">X=[x1,x2,…,xn]</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">min F(X)</span></div></div><div>这里的<img src="DogLeg.resources/F84B35AD-3EAA-4790-A524-4E6508A25570.gif" height="37" width="548"/><br/></div><div><span style="color: rgb(148, 17, 0);">F: object function</span></div><div><span style="color: rgb(148, 17, 0);">F: cost function</span></div><div><span style="color: rgb(148, 17, 0);">P: loss function</span></div><div><span style="color: rgb(148, 17, 0);">残差</span></div><div><br/></div><div><span style="font-weight: bold;">3.3 最小二乘的基本解法和分类</span></div><div><ul><li>线性：<span style="font-size: 14px; color: rgb(68, 68, 68); font-family: Tahoma, Arial, Helvetica, sans-serif; font-weight: bold;">残差(residual)是线性的（ri是线性的）</span></li><li>非线性：<span style="font-size: 14px; color: rgb(68, 68, 68); font-family: Tahoma, Arial, Helvetica, sans-serif; font-weight: bold;">残差(residual)是非线性的，ri是非线性的</span></li></ul><div><img src="DogLeg.resources/3671B827-F48B-4450-B865-10E93FACED50.png" height="496" width="1412"/><br/></div><div><br/></div><div><span style="color: rgb(68, 68, 68); font-family: Tahoma, Arial, Helvetica, sans-serif; font-weight: bold;">（1）线性的：直接套用公式</span></div></div><div><img src="DogLeg.resources/6A9BDFD4-C18D-452C-9F37-FE9AB9284B9E.png" height="440" width="1258"/><br/></div><div><br style="font-weight: bold; font-family: Tahoma; color: rgb(68, 68, 68);"/></div><div><span style="font-weight: bold;">（2）非线性的：利用2 中无约束优化的思想</span></div><div><br/></div><div><img src="DogLeg.resources/B4D9A7F5-CB1A-4474-8E32-D1C24C6593BE.gif" height="18" width="236"/><br/></div><div><br/></div><div><ul><li>Line Search</li><li>Trust Region</li></ul><div><img src="DogLeg.resources/085632F0-1701-4EC2-A114-AAF7F8C734DF.png" height="468" width="1422"/><br/></div></div><div>[Reference: ceres]</div><div><br/></div><div><span style="font-weight: bold;">3.4 最小二乘-线性搜索-</span><span style="font-weight: bold;">line search</span></div><div>参考2 <span style="background-color: rgb(255, 250, 165); color: rgb(68, 68, 68); font-family: Tahoma, Arial, Helvetica, sans-serif; font-weight: bold;-evernote-highlight:true;">无约束优化问题的求解方法。可以采用最速下降法、牛顿法、阻尼牛顿法。需要唯一需要注意的是这里的fi(X)是一个  平方函数 fi(X) = ri^2 </span></div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="background-color: rgb(255, 250, 165); color: rgb(68, 68, 68); font-family: Tahoma, Arial, Helvetica, sans-serif; font-weight: bold;-evernote-highlight:true;">所以 涉及的梯度和H矩阵都是针对</span><span style="background-color: rgb(255, 250, 165); color: rgb(68, 68, 68); font-family: Tahoma, Arial, Helvetica, sans-serif; font-weight: bold;-evernote-highlight:true;"> ri^2的</span></span></div><div><br/></div><div><span style="color: rgb(68, 68, 68); font-family: Tahoma, Arial, Helvetica, sans-serif; font-weight: bold;">3.4.1 最速下降法</span></div><div><span style="color: rgb(68, 68, 68); font-family: Tahoma, Arial, Helvetica, sans-serif; font-weight: bold;"> </span><img src="DogLeg.resources/9CB6E815-6930-4C8E-84B4-F29803659018.gif" height="21" width="358"/><br/></div><div><img src="DogLeg.resources/DCC86E80-CD0D-4946-A79B-A46792488445.gif" height="21" width="141"/><br/></div><div><img src="DogLeg.resources/12A893B9-5739-40FD-A524-E481BC0A5CFC.gif" height="21" width="221"/><br/></div><div>流程见第2部分 method1</div><div><br/></div><div><span style="color: rgb(68, 68, 68); font-family: Tahoma, Arial, Helvetica, sans-serif; font-weight: bold;">3.4.2 牛顿法</span></div><div><img src="DogLeg.resources/2D53BCDE-D7AC-41CB-BE1A-029205D21ACB.gif" height="21" width="523"/><br/></div><div><img src="DogLeg.resources/B23B9FF9-1A49-41DF-A584-9642C928C0B9.gif" height="21" width="213"/><br/></div><div><img src="DogLeg.resources/B4E12F4B-FA6C-4DE3-B3C2-D087FE89B24C.gif" height="21" width="230"/><br/></div><div>等价于Ax=b模型，线性方法的求解有很多方差，各种矩阵分解，如LU、cholesky分解等</div><div>缺陷还是计算量大，maybe H奇异不可逆</div><div>流程见第2部分 method2 or method3</div><div><br/></div><div><span style="font-style: italic;">梯度和海森矩阵具体求解，见笔记</span></div><div><img src="DogLeg.resources/94F7A485-1C3F-4790-B562-1E34F14D7BAC.jpg" height="1080" width="1440"/><br/></div><div><img src="DogLeg.resources/BD5A5759-627D-4DF8-82A6-D578E8ADC620.jpg" height="1080" width="1440"/><br/></div><div><br/></div><div><br/></div><div><span style="font-weight: bold;">3.4.3 高斯牛顿法</span></div><div>    高斯牛顿法解决非线性最小二乘问题的最基本方法，并且它只能处理二次函数。(使用时必须将目标函数转化为二次的)Unlike Newton’s method, the Gauss–Newton algorithm can only be used to minimize a sum of squared function values</div><div><br/></div><div><span style="font-weight: bold;">（1）理解方式一：针对ri对ri进行泰勒展开保留到一阶，</span><span style="font-weight: bold;">而不是||ri||^2 进行泰勒展开</span></div><div><img src="DogLeg.resources/CE89D944-4B2F-41DD-BEB9-AB49BD9D5179.gif" height="19" width="411"/><br/></div><div><img src="DogLeg.resources/00089437-9562-445F-B21B-6BAD68D8824C.gif" height="22" width="460"/><br/></div><div>                <img src="DogLeg.resources/090B0F76-7685-4FB3-9BDC-8E3BE9C322EF.gif" height="21" width="436"/>   (*)</div><div>式子中都已知，需要求解<img src="DogLeg.resources/41766F56-3983-482C-9AF1-CE29D9A2975F.gif" height="17" width="131"/> 。</div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">(*)是一个二次型函数，利用极值条件对(*)式子求导数</span></div><div><img src="DogLeg.resources/C83589D7-8B31-4F5F-A5B5-A8C563B663B5.gif" height="20" width="281"/><br/></div><div><img src="DogLeg.resources/93EE5387-1E26-4303-A3B1-F8FBA3EE0F37.gif" height="20" width="397"/> （增量方程、正规方程）</div><div>等价于Ax=b还是一个线性方法，求解这个线性方程得到<span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">得到驻点（必要条件） </span><img src="DogLeg.resources/41766F56-3983-482C-9AF1-CE29D9A2975F.gif" height="17" width="131"/><br/></div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">，而要满足是极小值点还需要满足充分条件：二阶Hession 矩阵正定，这里<img src="DogLeg.resources/26934E89-A8F8-4267-9286-21748459546B.gif" height="20" width="168"/><br/></span></div><div><br/></div><div style="box-sizing: border-box; padding: 8px; font-size: 12px; border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.14902);"><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">Method 4: 高斯牛顿法</span></div><div><br/></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">1. 选择初值</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">2. 对第k次迭代，求ri(xk)的梯度和误差函数ri(xk)</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">3. 求解增量方程得到 delta x ，可以采用矩阵分解方法</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;"> </span><img src="DogLeg.resources/93EE5387-1E26-4303-A3B1-F8FBA3EE0F37.gif" height="20" width="397"/><br/></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">4. 是否满足迭代停止条件</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">5. 否则得到</span><img src="DogLeg.resources/B4D9A7F5-CB1A-4474-8E32-D1C24C6593BE.gif" height="18" width="236"/><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;"> ，反复2-5</span></div><div><br/></div><div><br/></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">注意：</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">（1）这里的J是ri(x)的偏导数构成的梯度</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">（2）这里的H = J^T J ，要求是正定的，才能保证得到的 delta x是极值</span></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">（3）然而</span><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">J^T J 是一个半正定矩阵，实际情况中<span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">J^T J 会出现奇异和病态到时算法稳定性差不收敛情况</span></span></div><div><span style="font-size: 12px;"><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">（4）还是一个问题是如果求解出来的</span><img src="DogLeg.resources/41766F56-3983-482C-9AF1-CE29D9A2975F.gif" height="17" width="131"/><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco;">太大，就与在xk处泰勒展开的局部近似思想矛盾，不够准确也会导致不收敛-》</span><span style="font-size: 12px; color: rgb(148, 17, 0); font-family: Monaco; font-weight: bold;">由此引申出信頼域方法</span></span></div></div><div><br/></div><div>（2）理解方式2：参考上面笔记图2.这里的 H 是真实的ri(x)^2 的H矩阵的近似，忽略了真实H矩阵中的二阶项目，用一阶代替（用J(x)TJ(x)代替H的求法，从而节省了牛顿法的计算量）.<span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">但是这里必须满足条件是二阶可忽略的前提是：</span><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;"><span style="font-size: 14px; text-indent: 0px; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; top: 0px; clip: rect(1px, 1px, 1px, 1px); -webkit-user-select: none; background-color: rgb(255, 250, 165); overflow: hidden !important; line-height: normal;-evernote-highlight:true;">ri</span>比较小，或者 ri接近linear 这样一阶微分是常数，二阶微分为0</span></div><div><br/></div><div><br/></div><div><br/></div><div><span style="color: rgb(68, 68, 68); font-family: Tahoma, Arial, Helvetica, sans-serif; font-weight: bold;">3.5 最小二乘-信頼域方法-trust region</span></div><div><br/></div><div><span style="font-weight: bold;">3.5.1 LM方法：参考wangxi ligang之前的培训</span></div><div><a href="https://confluence.ygomi.com:8443/pages/viewpage.action?pageId=39127201" style="font-weight: bold;">https://confluence.ygomi.com:8443/pages/viewpage.action?pageId=39127201</a></div><div><br/></div><div style="box-sizing: border-box; padding: 8px; font-size: 12px; border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.14902);"><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">Method5 : LM方法</span></div><div><img src="DogLeg.resources/9C2CCFD1-5FA0-4B29-9D25-2832D8C472B9.png" height="520" width="982"/><br/></div><div><br/></div><div><br/></div><div><span style="text-indent: 0px; font-size: 14px; letter-spacing: normal; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(0, 0, 0); font-family: verdana; font-variant-caps: normal;">  采用拉格朗日乘子将该有约束优化问题- &gt;为无约束优化问题:</span></div><div style="text-indent: 0px; font-size: 14px; letter-spacing: normal; text-align: start; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; margin-top: 1em; margin-bottom: 1em;"><div><img src="DogLeg.resources/52F8544F-33B0-4B61-ACEC-E3BFD2521DF2.png" height="22" width="238"/><br/></div></div><div style="text-indent: 0px; font-size: 14px; letter-spacing: normal; text-align: start; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; margin-top: 1em; margin-bottom: 1em;"><span style="text-indent: 0px; font-size: 14px; letter-spacing: normal; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(0, 0, 0); font-family: verdana; font-variant-caps: normal;">     因为D=I，又有：</span></div><div style="text-indent: 0px; font-size: 14px; letter-spacing: normal; text-align: start; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; margin-top: 1em; margin-bottom: 1em;"><div><img src="DogLeg.resources/9CF505E4-5BF4-4FCA-85A0-3EF834743973.png" height="22" width="208"/><br/></div></div><div><span style="text-indent: 0px; font-size: 14px; letter-spacing: normal; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(0, 0, 0); font-family: verdana; font-variant-caps: normal;">       公式4.13就是LM算法的增量表达式。通过该式计算就可以避免掉高斯牛顿算法中出现的不收敛等问题，保证H正定</span></div></div><div><br/></div><div><br/></div><div style="box-sizing: border-box; padding: 8px; font-size: 12px; border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.14902);"><div><span style="font-size: 12px; color: rgb(68, 68, 68); font-family: Tahoma, Arial, Helvetica, sans-serif; font-weight: bold;">个人的理解：因为迭代法的主要思想是沿着函数下降方向走，但是高斯牛顿法求解的</span><img src="DogLeg.resources/6BFCB954-45CA-4581-9B3F-BA1DF702883D.gif" height="15" width="32"/><span style="font-size: 12px; color: rgb(68, 68, 68); font-family: Tahoma, Arial, Helvetica, sans-serif; font-weight: bold;"> 不一定能保证函数值下降</span><span style="font-size: 12px; color: rgb(68, 68, 68); font-family: Tahoma; font-weight: bold;">，预期下降值与实际下降值不相符，就需要改变方法或者不取那个迭代点。信頼域方法就是先限定一个范围让</span><img src="DogLeg.resources/6BFCB954-45CA-4581-9B3F-BA1DF702883D.gif" height="15" width="32"/><span style="font-size: 12px; color: rgb(68, 68, 68); font-family: Tahoma; font-weight: bold;">只能在这个区域内，就相当于加了一个约束条件，这个范围应该一开始比较小，因为是近似展开。</span></div><div><img src="DogLeg.resources/CE89D944-4B2F-41DD-BEB9-AB49BD9D5179.gif" height="19" width="411"/><br/></div><div><br/></div><div><span style="font-size: 12px; color: rgb(51, 51, 51); font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace;">然后在信頼域中</span></div><div><span style="font-size: 12px; color: rgb(68, 68, 68); font-family: Tahoma; font-weight: bold;">实际下降的值： <img src="DogLeg.resources/4F7CFAFC-9F10-4DBD-BD69-100DE88CD6F2.gif" height="18" width="162"/><br/></span></div><div><span style="font-size: 12px; color: rgb(68, 68, 68); font-family: Tahoma; font-weight: bold;">近似理论下降的值：<img src="DogLeg.resources/3240BCC4-1804-4C7F-BD20-46CDBEF3348E.gif" height="18" width="75"/><br/></span></div><div><br/></div><div><span style="font-size: 12px; color: rgb(68, 68, 68); font-family: Tahoma; font-weight: bold;">当实际下降的值  &gt; </span><span style="font-size: 12px; color: rgb(68, 68, 68); font-family: Tahoma; font-weight: bold;">近似理论下降的值 ：下降比较好，还可以放宽信頼域范围这样：</span><span style="font-size: 12px;"><span style="font-size: 12px; color: rgb(148, 17, 0); font-family: Tahoma; font-weight: bold;">加快收敛</span></span></div><div><span style="font-size: 12px; color: rgb(68, 68, 68); font-family: Tahoma; font-weight: bold;">当实际下降的值  &lt; </span><span style="font-size: 12px; color: rgb(68, 68, 68); font-family: Tahoma; font-weight: bold;">近似理论下降的值 ：说明近似比较差，再减小信頼域保证：</span><span style="font-size: 12px;"><span style="font-size: 12px; color: rgb(148, 17, 0); font-family: Tahoma; font-weight: bold;">能够收敛</span></span></div><div><br/></div><div><span style="font-size: 12px; color: rgb(148, 17, 0); font-family: Tahoma; font-weight: bold;">比值= 实际下降 / 预测下降</span></div><div><br/></div><div><br/></div><div><span style="font-size: 12px; color: rgb(148, 17, 0); font-family: Tahoma; font-weight: bold;">信頼域方法的思想：先确定步长范围，在范围内直接找到极小值点</span></div><div><span style="font-size: 12px; color: rgb(148, 17, 0); font-family: Tahoma; font-weight: bold;">线搜索方法思想：先确定搜索方向，再在该方向上找最佳步长</span></div></div><div><br/></div><div><br/></div><div><br/></div><div><span style="font-weight: bold;">3.5.2 dog-leg</span></div><div><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">1）</span><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">traditional-dogleg：</span><span style="background-color: rgb(255, 250, 165);-evernote-highlight:true;">思想是利用了最速下降和高斯牛顿的结合 </span></div><div><img src="DogLeg.resources/5B3F27D8-C390-4361-9EE3-FF841D35C3B6.png" height="491" width="559"/><img src="DogLeg.resources/09CA05BC-1225-474F-BB93-9CD34638FD0C.png" height="369" width="601"/><br/></div><div><br/></div><div><br/></div><div>推荐一个博客：<a href="https://blog.csdn.net/fangqingan_java/article/details/46956643">https://blog.csdn.net/fangqingan_java/article/details/46956643</a></div><ul><li>个人理解1：通过拉格朗日法将一个不等式约束问题转换为无约束优化问题，然后再用KKT条件可以知道信頼域范围与最优解之间的关系。就是说当最优解本来就在信頼域范围内，其实信頼域对最优解不起约束，那么最优解=就是求得的高斯牛顿的解。而当最优解在信頼域外部最优解被信頼域范围约束，在信頼域边缘上找到一个近似的最优解。</li></ul><div><br/></div><div><br/></div><ul><li>个人理解2：与LM相比Dogleg的关键优势在于：如果选择的信頼域大小会导致函数值实际下降量少，就是说近似的不准确，那么LM会重新减小信頼域，重新计算. 但是Dogleg 利用最速下降和高斯牛顿思想，保证得到的近似点在信頼域范围上，只需要计算高斯牛顿和柯西矢量之间的值.</li></ul><div><br/></div><div> </div><div style="box-sizing: border-box; padding: 8px; font-size: 12px; border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.14902);"><div><img src="DogLeg.resources/D25AF16A-2011-4C81-A041-49424759A5A4.png" height="489" width="573"/><br/></div></div><div><br/></div><div>还有其他dog-leg的扩展方法，此处不再讲</div><div><br/></div><div><br/></div><div><a href="https://blog.csdn.net/lucylove3943/article/details/41588491">https://blog.csdn.net/lucylove3943/article/details/41588491</a></div><div><a href="https://blog.csdn.net/hlx371240/article/details/39676003">https://blog.csdn.net/hlx371240/article/details/39676003</a></div><ol style="list-style-type: decimal; font-size: 14px; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><li><br/><span style="font-size: 14px; color: rgb(51, 51, 51); font-family: Arial, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">若高斯牛顿的结果在信頼域内，说明信頼域的约束不起作用</span></li><li><span style="font-size: 14px; color: rgb(51, 51, 51); font-family: Arial, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">若高斯牛顿结果再信頼域外，计算最速下降法，结果再信頼域外，说明沿着负梯度方向下降最多，直接取信頼域边缘上的点即可</span></li><li><span style="font-size: 14px; color: rgb(51, 51, 51); font-family: Arial, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">若高斯牛顿结果再信頼域外，计算最速下降法，结果再信頼域内，说明沿着负梯度方向下降多，而结果又在信頼域内部，我们可以让其更向着高斯牛顿方向再下降一点点，这样子可以加快收敛，于是最优的结果在两个结果的连线上。</span></li></ol><div><br/></div><div><br/></div><div><hr/><div><br/></div></div><div style="padding: 0px; font-size: 14px; letter-spacing: normal; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px; -webkit-text-stroke-width: 0px; margin-top: 1em; margin-bottom: 1em;"><div><img src="DogLeg.resources/B0C9EAAE-C367-4F51-A91B-E24595DA62D8.png" height="1166" width="2654"/><br/></div></div><div><br/></div><div><br/></div><div><br/></div></body></html>