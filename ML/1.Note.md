监督学习

无监督学习

两者的区别为**是否需要人工参与数据标注**



Week1

##  1.3监督学习(Supervised Learning)

教计算机如何去完成预测任务==（有反馈）==，<u>预先给一定数据量的输入</u>和<u>对应的结果</u>，建模拟合，最后让计算机预测未知数据的结果。

##### 1.回归问题(Regression): 回归问题即为预测一系列的==**连续值**==

在房屋价格预测的例子中，给出了一系列的房屋面基数据，根据这些数据来预测任意面积的房屋价格。给出照片-年龄数据集，预测给定照片的年龄。

**2.分类问题(Classification)**:分类问题即为预测一系列的==离散值==

即根据数据预测被预测对象属于哪个分类,视频中举了癌症肿瘤这个例子，针对诊断结果，分别分类为良性或恶性。还例如垃圾邮件分类问题，也同样属于监督学习中的分类问题





## 1.4无监督学习(Unsupervised Learning)

相对于监督学习，训练集不会有人为标注的结果==（无反馈）==，我们不会给出结果或无法得知训练集的结果是什么样，而是单纯由计算机通过无监督学习算法自行分析，从而“得出结果”。计算机可能会把特定的数据集归为几个不同的簇，故叫做聚类算法。

1.比如聚类问题

 





## 2单变量线性回归(Linear Regression with One Variable)

==个人理解：给定一个问题模型，即：给定了输入x和输出y，预测结果，预测量是连续的，是一个线性回归问题。通过输入输出可以拟合出很多假设的模型H(x)。再引入(Cost Function)概念，用于度量建模误差：==  ==**个人理解这里的(Cost Function)，即最小化预测模型与实际给定训练集合之间的误差。 通常会采用平方损失函数（最小二乘法）** **注意与loss function 的区别==**

![image-20181220133357005](/Users/test/Downloads/7-TestCode/__notebook/ML/image-20181220133357005-5284037.png)

==线性回归函数求解最小值问题属于**凸函数优化问题**。==





损失函数**(Loss/Error Function): 计算**单个**样本的误差==  [link](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/yWaRd/logistic-regression-cost-function)

==代价函数(Cost Function): 计算整个训练集所有损失函数之和的平均值==

机器学习中的目标函数、损失函数、代价函数有什么区别？- 知乎





## 2.5梯度下降

梯度下降背后的思想是：开始时，我们随机选择一个参数组合即起始点 ，计算损失函数失函数，然后寻找下一个能使得损失函数下降最多的参数组合。不断迭代，直到找到一个局部最小值(local minimum)，由于下降的情况只考虑当前参数组合周围的情况，所以无法确定当前的局部最小值是否就是全局最小值(global minimum)，不同的初始参数组合，可能会产生不同的局部最小值。

==个人理解：简单的梯度下降算法，步长太小收敛可能比较慢，步长太大收敛可能越过最低点，导致无法收敛==



**区别于线性回归算法，逻辑回归算法是一个分类算法，其输出值永远在 0 到 1 之间，即 $h_\theta(x) \in (0,1)$。**

假设函数：逻辑回归模型中，$h_\theta \left( x \right)$ 的作用是，根据输入 $x$ 以及参数 $\theta$，计算得出”输出 $y=1$“的可能性(estimated probability)。以肿瘤诊断为例，$h_\theta \left( x \right)=0.7$ 表示病人有 $70\%$ 的概率得了恶性肿瘤。





Week3

# 7 正则化(Regularization)

## 7.1 过拟合问题(The Problem of Overfitting)

对于拟合的表现，可以分为三类情况：

- **==欠拟合(Underfitting)==**

  无法很好的拟合训练集中的数据，**预测值和实际值的误差很大，这类情况被称为欠拟合**。拟合模型比较简单（特征选少了）时易出现这类情况。类似于，你上课不好好听，啥都不会，下课也差不多啥都不会。

- **==优良的拟合(Just right)==**

  不论是训练集数据还是不在训练集中的预测数据，都能给出较为正确的结果。类似于，学霸学神！

- **==过拟合(Overfitting)==**

  **能很好甚至完美拟合训练集中的数据，即 $J(\theta) \to 0$，但是对于不在训练集中的新数据，预测值和实际值的误差会很大，泛化能力弱，这类情况被称为过拟合。**拟合模型过于复杂（特征选多了）时易出现这类情况。类似于，你上课跟着老师做题都会都听懂了，下课遇到新题就懵了不会拓展。

线性模型中的拟合情况(左图欠拟合，右图过拟合)：
![](/Users/test/Downloads/7-TestCode/ML-AndrewNg-Notes/images/20180112_091654.png)

逻辑分类模型中的拟合情况：
![](/Users/test/Downloads/7-TestCode/ML-AndrewNg-Notes/images/20180112_092027.png)



为了度量拟合表现，引入：

- ==偏差(bias)==

  指模型的预测值与真实值的**偏离程度**。偏差越大，预测值偏离真实值越厉害。**<u>偏差低意味着能较好地反应训练集中的数据情况。</u>**

- ==方差(Variance)==

  指模型预测值的**离散程度或者变化范围**。方差越大，数据的分布越分散，函数波动越大，泛化能力越差。**<u>方差低意味着拟合曲线的稳定性高，波动小。</u>**

据此，我们有对同一数据的各类拟合情况如下图：
![](/Users/test/Downloads/7-TestCode/ML-AndrewNg-Notes/images/20180112_085630.png)

**==据上图，高偏差意味着欠拟合，高方差意味着过拟合。==**

==我们应尽量使得拟合模型处于低方差（较好地拟合数据）状态且同时处于低偏差（较好地预测新值）的状态。==

避免过拟合的方法有：

- 减少特征的数量
  - 手动选取需保留的特征
  - 使用模型选择算法来选取合适的特征(如 PCA 算法)
  - 减少特征的方式易丢失有用的特征信息
- 正则化(Regularization)
  - 可保留所有参数（许多有用的特征都能轻微影响结果）
  - 减少/惩罚各参数大小(magnitude)，以减轻各参数对模型的影响程度
  - 当有很多参数对于模型只有轻微影响时，正则化方法的表现很好
  - 

**==个人理解：正则化的本质就是，给优化参数一定约束，所以正则化与加限制约束，只是变换了一个样子而已。==**