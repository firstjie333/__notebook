监督学习

无监督学习

两者的区别为**是否需要人工参与数据标注**





##  1.3监督学习(Supervised Learning)

教计算机如何去完成预测任务==（有反馈）==，<u>预先给一定数据量的输入</u>和<u>对应的结果</u>，建模拟合，最后让计算机预测未知数据的结果。

##### 1.回归问题(Regression): 回归问题即为预测一系列的==**连续值**==

在房屋价格预测的例子中，给出了一系列的房屋面基数据，根据这些数据来预测任意面积的房屋价格。给出照片-年龄数据集，预测给定照片的年龄。

**2.分类问题(Classification)**:分类问题即为预测一系列的==离散值==

即根据数据预测被预测对象属于哪个分类,视频中举了癌症肿瘤这个例子，针对诊断结果，分别分类为良性或恶性。还例如垃圾邮件分类问题，也同样属于监督学习中的分类问题





## 1.4无监督学习(Unsupervised Learning)

相对于监督学习，训练集不会有人为标注的结果==（无反馈）==，我们不会给出结果或无法得知训练集的结果是什么样，而是单纯由计算机通过无监督学习算法自行分析，从而“得出结果”。计算机可能会把特定的数据集归为几个不同的簇，故叫做聚类算法。

1.比如聚类问题

 



## 2单变量线性回归(Linear Regression with One Variable)

==个人理解：给定一个问题模型，即：给定了输入x和输出y，预测结果，预测量是连续的，是一个线性回归问题。通过输入输出可以拟合出很多假设的模型H(x)。再引入(Cost Function)概念，用于度量建模误差：==  ==**个人理解这里的(Cost Function)，即最小化预测模型与实际给定训练集合之间的误差。 通常会采用平方损失函数（最小二乘法）** **注意与loss function 的区别==**

![image-20181220133357005](/Users/test/Downloads/7-TestCode/__notebook/ML/image-20181220133357005-5284037.png)

==线性回归函数求解最小值问题属于**凸函数优化问题**。==





损失函数**(Loss/Error Function): 计算**单个**样本的误差==  [link](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/yWaRd/logistic-regression-cost-function)

==代价函数(Cost Function): 计算整个训练集所有损失函数之和的平均值==

机器学习中的目标函数、损失函数、代价函数有什么区别？- 知乎





## 2.5梯度下降

梯度下降背后的思想是：开始时，我们随机选择一个参数组合即起始点 ，计算损失函数失函数，然后寻找下一个能使得损失函数下降最多的参数组合。不断迭代，直到找到一个局部最小值(local minimum)，由于下降的情况只考虑当前参数组合周围的情况，所以无法确定当前的局部最小值是否就是全局最小值(global minimum)，不同的初始参数组合，可能会产生不同的局部最小值。

==个人理解：简单的梯度下降算法，步长太小收敛可能比较慢，步长太大收敛可能越过最低点，导致无法收敛==



**区别于线性回归算法，逻辑回归算法是一个分类算法，其输出值永远在 0 到 1 之间，即 $h_\theta(x) \in (0,1)$。**

假设函数：逻辑回归模型中，$h_\theta \left( x \right)$ 的作用是，根据输入 $x$ 以及参数 $\theta$，计算得出”输出 $y=1$“的可能性(estimated probability)。以肿瘤诊断为例，$h_\theta \left( x \right)=0.7$ 表示病人有 $70\%$ 的概率得了恶性肿瘤。



